project_id: lmic-dev-datahub
location: us-east4

datasets:
  canonical: llm_cip_noc_canonical
  ops: llm_cip_noc_ops

tables:
  pairs: cip_noc_pairs
  cip_canonical: cip_canonical
  noc_canonical: noc_canonical
  run_registry: run_registry
  results: results

llm:
  base_url: "https://ollama-gcs-llama3-gpu-228291553312.us-east4.run.app"

  model: "llama3:8b"

  # Give enough time for cold starts + long generations
  timeout_s: 1800

  # Be patient with Cloud Run "service not ready yet" 503s
  max_retries: 12

  # Keep low until you confirm stability
  concurrency: 2
  batch_size: 50

  # Optional but strongly recommended once you add the code:
  # stagger_s: 10         # shard i waits i*10s before starting
  # ready_retries: 30     # readiness probe attempts
  # ready_sleep_s: 10     # seconds between readiness attempts

  impersonate_service_account: null

run_defaults:
  n_shards: 10
  prompt_profile: "balanced"