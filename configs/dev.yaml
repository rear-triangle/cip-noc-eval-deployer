project_id: lmic-dev-datahub
location: us-east4

datasets:
  canonical: llm_cip_noc_canonical
  ops: llm_cip_noc_ops

tables:
  pairs: cip_noc_pairs
  cip_canonical: cip_canonical
  noc_canonical: noc_canonical
  run_registry: run_registry
  results: results

llm:
  base_url: "https://ollama-gcs-llama3-gpu-228291553312.us-east4.run.app"

  # NEW: explicit auth mode (default is oidc)
  # - "oidc": send Authorization: Bearer <ID_TOKEN>
  # - "none": if the Cloud Run service is public (no auth)
  auth_mode: "oidc"

  # Ollama settings
  model: "llama3:8b"
  timeout_s: 300
  max_retries: 4

  # Worker runtime settings
  concurrency: 16
  batch_size: 50

  # Local token minting via impersonation (optional)
  impersonate_service_account: null

run_defaults:
  n_shards: 50
  prompt_profile: "balanced"