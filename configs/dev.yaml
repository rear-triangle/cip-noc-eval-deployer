project_id: lmic-dev-datahub
location: us-east4

datasets:
  canonical: llm_cip_noc_canonical
  ops: llm_cip_noc_ops

tables:
  pairs: cip_noc_pairs
  cip_canonical: cip_canonical
  noc_canonical: noc_canonical
  run_registry: run_registry
  results: results

llm:
  base_url: "https://ollama-gcs-llama3-gpu-228291553312.us-east4.run.app"
  model: "llama3:8b"
  generate_path: "/api/generate"

  timeout_s: 1800
  max_retries: 12

  concurrency: 1
  batch_size: 50

  stagger_s: 5
  ready_retries: 30
  ready_sleep_s: 5

  # NEW: generation controls
  temperature: 0.1

  # Cap output tokens (prevents rambles; too low can truncate JSON)
  num_predict: 220

  # CSV list used by llm_client to build stop sequences
  # Keep ``` to kill code fences; extra Rationale stops get appended in code
  stop_csv: "```"

  impersonate_service_account: null

run_defaults:
  n_shards: 10
  prompt_profile: "balanced"